{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started","text":"<p>Autolume is a no-coding generative AI system allowing artists to train, craft, and explore their own models.</p> <p>Explore features and published artworks on the Autolume website.</p>"},{"location":"#platform-support","title":"Platform support","text":"<p>Autolume is currently supported on Windows 10 and 11 with experimental support on Ubuntu 24.04.</p> <p>For best performance, an Nvidia GPU is required (RTX 2070 or higher recommended).</p> <p>Windows users must follow the Windows requirements guide for proper training performance.</p>"},{"location":"#downloading-autolume","title":"Downloading Autolume","text":"<p>Get the latest release from the official website: Download at metacreation.net</p>"},{"location":"live-module/","title":"Live Module","text":"<p>Autolume Live is the module in which you can load a model and work with it in real time. It provides you with a diverse set of features to explore the latent space in different ways and to play with the parameters of the network. It also enables you to control the parameters via OSC protocol for audio-reactive and other kinds of interactive works.</p>"},{"location":"live-module/#osc","title":"OSC","text":"<p>Autolume-live allows users to control parameters using its interface but also provides the option to control parameters using OSC. This allows you to control parameters in Autolume-live using other software, such as TouchDesigner, Max/MSP, or Processing. The OSC interface is available for all parameters in the real-time interface. Most widgets have an OSC menu similar to the following:</p> <p></p> <p>It displays the different parameters that can be controlled using OSC. By clicking on a parameter name, a popup opens which allows you to check the box Use OSC\u00a0to activate the OSC control for that parameter. In the next field, you can provide the OSC channel name. The last field is a mapping function that can be used to perform post-processing on the control signal to make sure that the right data range is used. This follows the math format defined by Python.</p> <p></p> <p>Please see Performance and OSC\u00a0about how to set the OSC IP Address and port in the software you use to send the OSC signals.</p>"},{"location":"live-module/#network-and-latent","title":"Network and Latent","text":"<p>This widget allows you to load models and explore their latent space, i.e. the images it can generate.</p>"},{"location":"live-module/#model-loading","title":"Model Loading","text":"<p>Use Find\u00a0to navigate to the location of a .pkl file or press Models\u00a0to open a list of models in your models\u00a0folder inside your autolumelive_colab\u00a0directory (the installation directory). You can copy any .pkl file there for quick access inside Autolume-live.</p> <p>Loading a model for the first time can take a few seconds as we test if the computer is capable of running custom CUDA code that improves the performance. Afterward, models are stored in cache and loading should be much faster.</p> <p></p> <ul> <li>Recent:\u00a0Shows the most recently loaded models.</li> <li>Find:\u00a0Opens a file browser to find a model.</li> <li>Models:\u00a0Shows all models in the \u201cmodels\u201d folder.</li> </ul>"},{"location":"live-module/#latent-space","title":"Latent Space","text":"<p>StyleGAN's latent space is a hyper-dimensional vector space where each point uniquely represents a potential image generated by the model. By changing and manipulating vectors in the latent space, it is possible to alter the characteristics of the generated images and continuously explore the space of possibilities. This widget allows you to explore this space. There are two ways of exploring the latent space:</p> <ol> <li>Seed</li> </ol> <p></p> <p>In this mode, the latent space is controlled using seed numbers, each corresponding to a unique latent vector.</p> <ul> <li>Project:\u00a0StyleGAN has a feature that maps latent vectors into a more desirable space. By toggling this off, you might receive less consistent results.</li> <li>Seed:\u00a0The seed is a number that is used to generate a latent vector. By changing the seed you can explore different points in the latent space.</li> <li>Drag:\u00a0By dragging after clicking the button you can explore the latent space. This incrementally changes the seed.</li> <li>Anim/Stop:\u00a0Starts/stops animating a continuous movement in the latent space.</li> <li> <p>Speed:\u00a0Controls the speed of the animation.</p> </li> <li> <p>Vector</p> </li> </ul> <p></p> <p>This mode corresponds to a more free exploration of the latent space, similar to a random walk, leading to a more fluid feeling when animating. Seed numbers are not available in this mode.</p> <ul> <li>Project:\u00a0StyleGAN has a feature that maps latent vectors into a more desirable space. By toggling this off, you might receive less consistent results.</li> <li>Randomize:\u00a0Randomizes the latent vector.</li> <li>Anim/Stop:\u00a0Starts/stops animating a continuous movement in the latent space.</li> <li>Speed:\u00a0Controls the speed of the animation.</li> <li>Save/Load Vector:\u00a0You can save a specific latent vector and later load it to replicate the same result.</li> </ul>"},{"location":"live-module/#diversity-and-noise","title":"Diversity and Noise","text":"<p>This widget allows users to manipulate the diversity of images that can be generated and the internal noise used by the model. The noise generally defines the texture of the image, while the diversity defines the overall look of the image.</p> <p></p> <ul> <li>Diversity:\u00a0The diversity of the image. This is a slider between 0 and 2 (but can be changed to any number through OSC). A value of 0 means that the model will generate the same image every time. Generally, a value between 0.8 and 1 is recommended for a wide range of images, that stay true to the original dataset.</li> <li>Noise:\u00a0Toggle the noise on and off. This will give you a feel of which features the noise impacts. A rule of thumb is that when the noise is turned off, the resulting images will be smooth, without textures.</li> <li>Global Noise:\u00a0This is a slider between 0 and 2 (but can be changed to any number through OSC) that controls the amount of noise that is applied to the entire image.</li> <li>Noise Seed:\u00a0This is an integer that controls the seed of the noise. This allows you to change the noise without changing the diversity of the image. This is useful when you want to keep the same image but change the texture.</li> <li>Anim:\u00a0Toggles noise animation, which changes the Noise Seed every frame.</li> </ul>"},{"location":"live-module/#looping","title":"Looping","text":"<p>To generate seamless loops use this widget. It can either work based on specified keyframes or by \"noise loop\", creating a random loop based on a set radius. Each can be animated based on a specific time interval or by a speed value.</p>"},{"location":"live-module/#keyframe-looping","title":"Keyframe Looping","text":"<ul> <li># of Keyframes:\u00a0The number of keyframes used to generate the loop. The more keyframes, the more accurate the loop will be.</li> <li>Keyframes:\u00a0This popup window controls the keyframes used to generate the loop. You can add keyframes by clicking the keyframe button which opens a popup. </li> <li>Time:\u00a0Controls the duration of the loops in seconds.</li> <li>Index:\u00a0Controls the keyframe number in the loop.</li> <li>Alpha:\u00a0A value between 0 and 1 that corresponds to the whole length of the loop.</li> </ul>"},{"location":"live-module/#noiseloop-looping","title":"NoiseLoop Looping","text":"<ul> <li>Time:\u00a0Controls the duration of the loops in seconds.</li> <li>Looping Seed:\u00a0Changes the start point of the loop.</li> <li>Radius:\u00a0Controls how \u201clarge\u201d the latent loop is. Larger values correspond to more diverse changes during the loop.</li> <li>Alpha:\u00a0A value between 0 and 1 that corresponds to the whole length of the loop.</li> </ul>"},{"location":"live-module/#perfect-loop","title":"Perfect Loop","text":"<p>The Perfect Loop option at the bottom of both looping options can help capture the perfect loop. Checking the box will send a 1 signal at the end of each loop cycle through OSC with the OSC IP, port, and address that you can specify.</p>"},{"location":"live-module/#performance-and-osc","title":"Performance and OSC","text":"<p>This widget provides information and settings about the real-time module\u2019s performance, OSC connection IP address and port, and NDI name for sending visuals out.</p> <p>The Render FPS\u00a0shows the frame rate of the images being generated. The NDI Name\u00a0can be used to receive the visuals in the software using the NDI protocol. The OSC information is to receive OSC signals in Autolume.</p> <p>At the bottom of the widget, you check the box for CPU, if you want to run on CPU only. This will allow you to still run the real-time module, but will significantly reduce the frame rate of image generation (to around 1 fps, depending on your specific CPU). You can also choose the Custom Kernel\u00a0option after a model is loaded to increase the performance and boost the rendering frame rate.</p>"},{"location":"live-module/#adjust-input","title":"Adjust Input","text":"<p>This widget allows you to move the latent vector in adjustable directions. By default, the sliders correspond to random directions, each controllable also through OSC. You can also use Browse (on the right) to load the vectors calculated from the GANSpace\u00a0module so that all the sliders correspond to the extracted features. It is also possible to load individual vectors for each slider or to randomize directions altogether or individually.</p>"},{"location":"live-module/#layer-transformations","title":"Layer Transformations","text":"<p>Under the hood the generative model has layers. Each layer performs operations on different resolutions. With this widget, you can control the different layers. This allows you to create a wide range of effects.</p> <p>You can either do so per resolution in the Simple mode:</p> <p></p> <p>Or per layer in the Advanced mode:</p> <p></p> <p>To select which layer you want to manipulate you press the second checkbox, with the pen at the respective resolution or layer. And instead of only showing the final image you can also display the different layers output by clicking the checkbox with the eye.</p> <p>Per layer, you can either perform a transformation on the image or specify the strength of the noise. Which layers are actively manipulated is visible by the T\u00a0and N\u00a0icons. Some layers do not have any adjustable Noise, hence the noise tab is grayed out for those layers.</p>"},{"location":"live-module/#transformation","title":"Transformation","text":"<p>You can apply so-called affine transformations to the layers, such as rotation, scaling, and translation, These can be performed to all features, a random amount or a set range.</p> <p></p> <p>Each transformation's parameter can be controlled by an appropriate interface or by OSC.</p>"},{"location":"live-module/#noise","title":"Noise","text":"<p>You can regulate the strength of the noise per layer.</p> <p></p>"},{"location":"live-module/#model-mixing-real-time","title":"Model Mixing (real-time)","text":"<p>This widget allows you to perform model mixing like the offline module (see Model Mixing). When you switch layers a short latency might arise.</p> <p></p> <p>The first checkbox allows you to toggle if the mixed model should be used or not.</p> <p>Following you can select a model either through the browse feature, by listing all models in the \"models\" folder. Or using the \"Find\" button to open a file browser.</p> <p>Lastly, you can write a model name into the text field following the buttons. This allows you to quickly save the mixed model into the \"models\" folder.</p> <p>After loading a second model you can mix the two models.</p> <p></p> <p>When mixing, you can choose to select features based on resolution or by layer. The latter option provides finer control, but the former option usually suffices. You can also remove features by pressing \"X.\" This can be useful when one of the models generates images of higher resolution, and you want to align the resolution with the lower-resolution model.</p>"},{"location":"live-module/#presets","title":"Presets","text":"<p>It is possible to save presets\u00a0of the current state of all widgets and load them again. This allows you to quickly switch between different settings.</p> <p>The checkboxes allow you to choose which preset you want to load or save to. If a checkbox is grayed out, it means nothing is saved to that preset.</p> <p>At the bottom you can define the path where the presets are saved to and loaded from. Using the \"Preset Path\" opens a file browser to select a folder. While the \"Recent\" button opens a popup with the most recently used folders.</p> <p>The OSC controls of this widget allow you to switch between presets in the current folder, by sending the name of the preset. Please note that you can rename your presets.</p>"},{"location":"offline-modules/","title":"Offline Modules","text":""},{"location":"offline-modules/#data-preparation","title":"Data Preparation","text":"<p>This module provides options for dataset preparation prior to model training. The Data Preparation Module can be used for advanced configuration, while the Quick Settings feature allows faster dataset creation with basic parameters. It supports importing image and video files, formatting data into a consistent structure required for training. When a video file is imported, frames are automatically extracted based on the specified frame rate, converting motion footage into a sequence of still images suitable for training.</p> <p></p>"},{"location":"offline-modules/#data-preparation-module","title":"Data Preparation Module","text":"<p>The Data Preparation Module offers extended functionality and finer control over dataset creation. It includes options such as non-square framing to preserve image aspect ratios, image augmentations to expand dataset size and diversity, and integrated tools for managing and organizing data directly within the application. This module enables data to be imported, refined, and compiled entirely in-app. A built-in preview panel displays preprocessing results in real time, allowing visual verification of all applied changes before preprocessing and saving.</p> <p></p>"},{"location":"offline-modules/#data-preparation-quick-settings","title":"Data Preparation Quick Settings","text":"<p>Unlike the full Data Preparation Module, the Quick Settings feature provides only the essential parameters needed for preparing data. It is designed for fast dataset creation and requires precompiled data, offering a simplified workflow for generating datasets quickly.</p>"},{"location":"offline-modules/#supported-file-formats","title":"Supported File Formats","text":"<p>The Data Preparation Module supports importing both image and video files. The following formats are supported:</p> <p>Image formats:</p> <ul> <li>PNG (.png)</li> <li>JPEG (.jpg, .jpeg)</li> <li>BMP (.bmp)</li> <li>WebP (.webp)</li> <li>GIF (.gif)</li> </ul> <p>Video formats:</p> <ul> <li>MP4 (.mp4)</li> <li>AVI (.avi)</li> <li>MOV (.mov)</li> <li>MKV (.mkv)</li> <li>WebM (.webm)</li> <li>GIF (.gif)</li> </ul> <p>Data Preparation Process</p> <p>To prepare a dataset, provide the following:</p> <ul> <li>Quick Settings:</li> <li>Data Path: Specifies the source directory containing the image and video files. All supported media within this directory will be imported automatically.</li> <li>Resize Mode:\u00a0Determines how images are resized during preprocessing. Options include stretch and center-crop.</li> <li>Resolution:\u00a0Sets the target pixel dimensions (width \u00d7 height) for all processed images using the + and \u2013 controls. Higher resolutions produce sharper outputs but require more GPU memory during training. It is recommended to test system limits before finalizing this setting.</li> <li>Folder Name: Defines the name of the output directory for the processed dataset. By default, the system appends a resolution suffix (e.g., dataset_1024x1024) to the folder name to indicate the dataset\u2019s resolution size.</li> <li>Directory Path: Specifies the destination path where the processed dataset folder will be created.</li> <li>Data Preprocessing Module:</li> <li>Non-square Framing: Enables image resizing while maintaining the original aspect ratio instead of forcing a square crop.<ul> <li>Aspect Ratio\u00a0(Height &amp; Width): Sets custom height and width proportions for non-square datasets.</li> <li>Padding Mode: Determines how empty areas are filled when adjusting aspect ratios.</li> </ul> </li> <li>Augmentation: Applies augmentation techniques to the imported images during processing. These transformations are used to increase dataset size. It is important to select augmentation options carefully based on the nature of the data; for example, if the dataset contains orientation-sensitive subjects such as human faces, flip operations should be adjusted accordingly.</li> </ul>"},{"location":"offline-modules/#training","title":"Training","text":"<p>The Training Module enables users to train models using datasets prepared through the Data Preparation Module. It supports both training from scratch and resuming from existing checkpoints. The module also leverages data augmentation during training to improve model stability and performance, particularly when working with smaller or less diverse datasets. Throughout the training process, checkpoints and image outputs are automatically generated to document progress and model development.</p> <p></p>"},{"location":"offline-modules/#dataset","title":"Dataset","text":"<p>The dataset size can range from fewer than a hundred to several thousand samples. A minimum of 50\u2013200 samples is recommended, depending on the diversity and variability of the data. For optimal results, all images within a dataset must have the same dimensions, as a consistent image size is required for model training. Resizing can be performed manually or handled automatically within Autolume using the data preparation module.</p>"},{"location":"offline-modules/#training-augmentation","title":"Training Augmentation","text":"<p>To facilitate training with smaller datasets, two augmentation methods are provided in Autolume: Adaptive Discriminator Augmentation (ADA) and Differentiable Augmentation for Data-Efficient GAN Training (DiffAug). These augmentation methods apply visual transformations to your data during training without modifying your original dataset to help the model continue learning effectively, even when limited data is available. Each method offers a set of pipelines, which are simply different combinations of these transformations applied dynamically during training.</p> <p>While both methods serve the same purpose, they differ in how they apply these transformations. DiffAug uses fixed-strength, randomized augmentations in every training batch, introducing consistent variation. In contrast, ADA adjusts the strength of augmentations automatically based on how well the model is learning. It's recommended to experiment with both methods and their pipelines to find what works best for your dataset.</p> <p>If training performance remains poor even after experimenting with augmentation methods and pipelines, consider increasing the size and diversity of your dataset. Additional data can significantly improve model stability and output quality. You may also use the Data Preparation Module\u2019s augmentation features to expand your dataset before training.</p> <p>For a detailed guide on choosing the right augmentation method and pipeline, refer to this document: How to Select Augmentation Method and Pipeline </p> <p>For more technical details, please refer to: ADA: https://github.com/NVlabs/stylegan2-ada-pytorch DiffAug: https://github.com/mit-han-lab/data-efficient-gans</p>"},{"location":"offline-modules/#training-process","title":"Training Process","text":"<p>To train a model, provide the following:</p> <ul> <li>Save Path:\u00a0Choose the location where the training files will be saved.</li> <li>Dataset:\u00a0Select the dataset on which the model will be trained. (See Dataset)</li> <li>Resume Pkl:\u00a0Resume training from a previous training run or a pre-trained model by selecting a .pkl file. Skip this field if you are training the model from scratch.</li> <li>Training Augmentation:\u00a0Select the style of augmentation to be applied during training. Experiment with different styles to find the one that works best for your data. (See Training Augmentation)</li> <li>Augmentation Pipeline:\u00a0Choose the types of augmentation to be performed. (See Choosing Augmentation Pipeline)</li> <li>Batch Size:\u00a0Determine the number of images processed at once. This value is limited by the GPU memory available. A batch size of 8 is recommended for NVIDIA RTX 2070 GPU and above. Reduce to lower values (minimum 2) if memory issues arise.</li> <li>Configuration:\u00a0Selects from a set of preset training configurations that serve as reliable starting points for different model resolutions. For example, the paper256 configuration is recommended when training models at 256\u00d7256 resolution. Refer to the Training New Networks\u00a0section of this link\u00a0for more details on the available configurations.</li> </ul> <p>Advanced Options:</p> <ul> <li>Generator and Discriminator Learning Rates:\u00a0Keeping the default value (0.002) is recommended for most cases. If changing all other hyperparameters does not lead to successful training, you can lower these values to 0.001.</li> <li>Gamma:\u00a0Experiment with different gamma values if the default value (10) is not leading to successful training results. We recommend trying out at least a few different values of Gamma for each new dataset. Using increments of 10 is recommended.</li> <li>Number of ticks between snapshots:\u00a0Specifies how often image snapshots and checkpoint files are generated during training. Each tick represents a defined training interval. Lower values produce snapshots more frequently, allowing closer monitoring of progress, but the visual changes between snapshots will be smaller. A setting between 1 and 4 is generally recommended for balanced tracking and performance.</li> </ul> <p>After selecting the desired options, click the Train button to begin the training process. During training, the model periodically saves checkpoint (.pkl) files to the specified Save Path. A preview window displays a grid of randomly generated images from the model, corresponding to the latest image snapshot files saved in the fakesXXXXXX format (e.g., fakes000352) in the same directory.</p> <p>Training can be stopped at any time by clicking the Cancel button in the preview window. The final model is saved as a .pkl file in the specified Save Path, and progress can be tracked through the series of generated image snapshots.</p>"},{"location":"offline-modules/#additional-resources","title":"Additional Resources","text":"<p>Please refer to these resources for more advanced discussions about hyperparameters for training.</p> <p>StyleGan Deep Dive </p> <p>Jules Padova</p> <p>Training configurations </p> <p>Tero Karras &amp; Janne Hellsten (NVIDIA Research)</p>"},{"location":"offline-modules/#alternative-training-options","title":"Alternative Training Options","text":"<p>While Autolume provides the necessary features for network training on a local machine, for more serious training sessions, users can use cloud-based alternative options to train a StyleGAN2 model on more powerful GPUs. The trained model (either using Tensorflow or PyTorch) can then be used in Autolume-live for real-time use or in other modules. A number of alternative options are as follows:</p> <ul> <li>Google Colab (simple, both free and paid options)\u00a0</li> <li>Colab notebook 1</li> <li>Colab notebook 2</li> <li>Paperspace (advanced, paid)</li> <li>How to Run StyleGAN2-ADA-PyTorch on Paperspace</li> </ul>"},{"location":"offline-modules/#projection-module","title":"Projection Module","text":"<p>The projection module allows loading a target image, a text prompt, or a combination of both, \u00a0to find the closest corresponding latent vector of a trained model. Different methods are provided to search for the corresponding latent vector that you can choose from. The output is the calculated vector plus a process video showing the search process.</p> <p></p> <p>The following options are available for the projection module:</p> <ul> <li>Models:\u00a0Select the .pkl file of the model used for projection.</li> <li>Target Image:\u00a0Choose a single image that should be found in the model.</li> <li>Target Text:\u00a0Provide a description of an image that should be found in the model.</li> <li>Save Path:\u00a0Specify the path where the closest match should be saved. The saved result includes the position in the model and the closest match.</li> <li>Save Video:\u00a0Select this option to save a video of the process of finding the closest match.</li> <li>Seed:\u00a0Select a seed to ensure consistent results for the closest match.</li> <li>Learning Rate:\u00a0Adjust the learning rate of the projection to control convergence speed. Higher values result in faster projections but may be less accurate.</li> <li>Steps:\u00a0Determine the number of steps for the projection. Higher values increase accuracy but require more time.</li> <li>Use VGG:\u00a0Enable this option to use the VGG16 network for calculating image distance based on general features rather than pixel-by-pixel comparisons.</li> <li>Use CLIP:\u00a0Enable this option to use the CLIP network for calculating image and text distance based on general features rather than pixel-by-pixel comparisons.</li> <li>Use Pixel:\u00a0Enable this option to use pixel distance for image comparison. This puts more weight on pixel similarity between the match and the target.</li> <li>Use Penalty:\u00a0Enable this option to penalize large update steps, resulting in a smoother projection and avoiding local minimums.</li> <li>Use Center:\u00a0Enable this option to use an additional center crop as the target image. This can improve matching accuracy but may reduce overall accuracy.</li> </ul>"},{"location":"offline-modules/#ganspace","title":"GANSpace","text":"<p>This module identifies and extracts interpretable feature directions in the latent space of a trained model based on Principal Component Analysis (PCA). These extracted feature directions can be then accessible in the Renderer (see Adjust Input).</p> <p></p> <p>The module provides the following options:</p> <ul> <li>Models:\u00a0Select the .pkl file of the model for GANSpace exploration.</li> <li>PCA Estimator:\u00a0Choose an algorithm to find salient directions in the latent space. Different algorithms result in slightly different extracted directions. Experiment with each option to find the best fit for your model.</li> <li>Features:\u00a0Specify the number of features to extract. More features result in more adjustable directions in the real-time interface but also increase file size. Often, extracting around 10 features is recommended.</li> <li>Sparsity:\u00a0This parameter adjusts the sparsity of the extracted directions. Leave it at the default if you are unsure about its impact.</li> <li>Save Path:\u00a0Choose the path to save the GANSpace results, which include the extracted directions. These directions can be loaded in the \"Adjust Input\" widget in the real-time interface.</li> </ul> <p>For more technical information on GANSpace, please refer to the following resources:</p> <p>GANSpace Paper</p> <p>GANSpace Repository on GitHub</p>"},{"location":"offline-modules/#super-resolution","title":"Super-resolution","text":"<p>The Super-resolution module provides the feature to upscale images and videos for higher resolution distribution of your work. There are three different models available: Quality, Balance, and Fast. While the Processing speeds of the models differ, Quality being the slowest and Fast being the fastest, the slower models tend to preserve more details from the original image/video. However, the quality of the results also depends on the type of visual content and is better to be tested with all three models to find the desired option.</p> <p></p> <p>The following options are available:</p> <ul> <li>Input Files:\u00a0Select the path to the images or videos that should be upscaled. Multiple files can be selected at once.</li> <li>Result Path:\u00a0Specify the path where the upscaled images or videos should be saved.</li> <li>Model:\u00a0Choose the upscaling model to be used: Fast, Balanced, or Quality. Fast is the quickest but provides lower quality, while Quality is the slowest but provides the highest quality.</li> <li>Scale Mode:\u00a0Choose between defining the exact Height\u00a0and Width\u00a0of the output or using a Scale Factor. If the scale factor is selected, the height and width will be automatically calculated based on the scale factor.</li> <li>Sharpening:\u00a0Adjust the sharpness of the upscaled image. Sharpening=1 means no added sharpness. Higher values result in sharper images but may introduce artifacts.</li> </ul>"},{"location":"offline-modules/#model-mixing","title":"Model Mixing","text":"<p>This module provides the ability to mix two trained models to make a new model. It works based on selecting parts of one model and parts of a different model, thereby mixing the features of the two models.</p> <p></p> <p>To mix two models, specify the following:</p> <ul> <li>Model 1: Select the .pkl file of the first model to be mixed.</li> <li>Model 2: Select the .pkl file of the second model to be mixed.</li> </ul> <p>After pressing \"Combine,\" you can select the layers to be used from each model. The layers are listed in order, from early to late layers. While lower-resolution layers (e.g. 4x4, 8x8, 16x16) correspond to coarse and higher-level features, higher-resolution layers correspond to fine features and textures. This can be used to determine what type of features the mixed model inherits from each source. Each layer can also be expanded to select detailed components for advanced mixing. Additionally, features can be removed from the mix. This can be useful when one of the models generates images of higher resolution, and you want to align the resolution with the lower-resolution model.</p> <p></p> <p>Finally, you have the option to save the mixed model as a new .pkl file, which can be used anywhere in Autolume.</p> <p>For real-time model mixing in the Renderer, see Model Mixing (real-time).</p>"},{"location":"training-aug/","title":"Training Augmentation","text":""},{"location":"training-aug/#diffaug-and-ada","title":"DiffAug and ADA","text":"<p>DiffAug and ADA are image augmentation strategies applied during training to help the model learn effectively when trained on a smaller or limited dataset. By introducing controlled variations to the training images, these methods allow the model to learn more robustly from smaller datasets. Each method uses its augmentation pipeline, applying techniques such as flipping, colour shifts, or geometric changes to improve training outcomes.</p> <p>It's important to note that these augmentations are not applied directly to your dataset files. Instead, they act like a lens during training. When the model \"sees\" your images, it sees an augmented version based on the chosen pipeline. Your original data remains unchanged.</p>"},{"location":"training-aug/#choosing-augmentation-method","title":"Choosing Augmentation Method","text":"<p>Choosing between DiffAug and ADA primarily depends on two factors: the size of your dataset and the training goal. It's important to consider both before beginning the training process.</p> Criteria DiffAug ADA Dataset Size Small to medium dataset (fewer than 10k images) Works well with all sizes Training Speed Faster Slower Variation Creates more visual variation in the final model More consistent visuals compared to the dataset Best For Quick experiments, smaller datasets, and cases where variety is welcome Long, stable training sessions with a focus on dependable model output quality"},{"location":"training-aug/#choosing-augmentation-pipeline","title":"Choosing Augmentation Pipeline","text":"<p>A general rule of thumb, the more data you have, the fewer augmentation techniques you need, which also means faster training. The pipeline refers to the combination of augmentation techniques applied during training to help the model learn better. Choosing the right pipeline would depend on your dataset.</p>"},{"location":"training-aug/#diffaug-pipeline","title":"DiffAug Pipeline","text":"<p>When choosing a pipeline for DiffAug, consider how much variation your dataset can realistically tolerate. Since DiffAug applies augmentations on each training batch, it introduces more visual variety, which can help the model generalize better, but it can also shift the dataset from its original form. Below are quick guidelines to help you choose the right components:</p> <ul> <li>Colour</li> <li>Ideal for datasets captured under various lighting conditions or environments.</li> <li>Avoid if your dataset is colour-sensitive, like artwork where colour accuracy matters.</li> <li>Translation</li> <li>Useful when the subject appears in different positions across the dataset.</li> <li>Avoid if spatial consistency is important (e.g., faces or aligned subjects)</li> <li>Cutout</li> <li>Helps the model learn to focus on different parts of the image by masking random regions.</li> <li>Avoid if your dataset is very small or if the subject takes up only a small part of the image, as critical details might be lost.</li> </ul>"},{"location":"training-aug/#ada-pipeline","title":"ADA Pipeline","text":"<p>When choosing an ADA pipeline, the most important factor is your dataset size. Each augmentation technique has different benefits depending on how much data you're working with. A good starting point for most cases is the default 'bgc' pipeline. As a general rule, the more data you have, the less augmentation you need. For datasets with 10k images or fewer, 'bgc' helps maintain stability during training. When your dataset exceeds 100k images, it's often best to simplify the pipeline by using only 'bg', or even disabling augmentations altogether (to be implemented). Ultimately, every dataset behaves differently, so it's always best to experiment and adjust based on your training results.</p> <p>Below is a breakdown of each augmentation component used in ADA pipelines:</p> <ul> <li>Blit (b): Flips image to left/right, 90\u00b0 interval rotations, and translation.</li> <li>Geometric (g): Zooms the image in/out, rotates the image (up to 360\u00b0), and stretches or squishes the image.</li> <li>Colour (c): Perform brightness, contrast, and colour tone, and saturation changes to simulate different lighting.</li> <li>Filter (f): Sharpens or blurs the image by adjusting textures.</li> <li>Noise (n): Adds noise to the image.</li> <li>Cutout (c): Covers a random part of the image using a patch.</li> </ul> <p>Note: The pipeline is defined using combinations of these letters. For example, 'bgcfnc' includes Blit, Geometric, Colour, Filter, Noise, and Cutout augmentations.</p>"},{"location":"training-aug/#diving-deeper-technical-breakdown-of-augmentation-methods","title":"Diving Deeper: Technical Breakdown of Augmentation Methods","text":"<p>Both DiffAug and ADA perform augmentations during training to help models generalize better. This is especially useful when working with smaller datasets or with unbalanced ones where certain visual styles are underrepresented. However, the two methods operate in fundamentally different ways. DiffAug applies constant augmentations per batch, introducing consistent randomness throughout training, which can lead to greater diversity in generated outputs. In contrast, ADA augments data based on the discriminator's confidence, adaptively increasing or decreasing augmentation strength over time to maintain training stability. This difference in augmentation strategy not only affects the training dynamics but also the visual consistency and convergence behavior of the final model.</p>"},{"location":"training-aug/#difference-between-diffaug-and-ada","title":"Difference between DiffAug and ADA","text":"Category DiffAug ADA Purpose Applies fixed, differentiable augmentations to each batch of real and fake images Dynamically adjusts augmentation strength to prevent discriminator overfitting during training Applies To Both real and fake images. Both real and fake images. Augmentation Strength Fixed strength with randomized augmentations applied uniformly across training Adaptive strength which adjusts in response to the discriminator's performance during training Augmentation Types \u2022 Colour (brightness, saturation, contrast)\u2022 Translation (X, Y shift)\u2022 Cutout (random masking) \u2022 Blit (pixel shifts, flips, 90\u00b0 rotations)\u2022 Geometric (zoom, rotate, stretch)\u2022 Colour\u2022 Filter (blur/sharpen)\u2022 Noise\u2022 Cutout <p>Unlike DiffAug, which applies fixed augmentations to every batch from the start of training, ADA adjusts augmentation strength dynamically over time. It begins with an augmentation probability $p = 0$, allowing the discriminator to initially learn from unaltered real and generated images. As training progresses and the discriminator becomes increasingly confident, risking overfitting, ADA gradually increases $p$, making augmentations stronger to regularize the training. This adaptive mechanism helps prolong stable training by preventing the discriminator from memorizing the dataset. However, if $p$ becomes too high, the augmentations can start to degrade the quality of training. Therefore, it's important to experiment with the generated .pkl checkpoints along the way to find the best model output for your needs.</p> <p>For more technical details, please refer to:</p> <ul> <li>DiffAug: https://arxiv.org/pdf/2006.10738</li> <li>ADA: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/ada-paper.pdf</li> </ul>"},{"location":"training-aug/#augmentation-examples","title":"Augmentation Examples","text":""},{"location":"training-aug/#diffaug-transformation-previews","title":"DiffAUG Transformation Previews","text":"<ul> <li> <p>Colour </p> <p></p> <p>Colour transformations adjust brightness, saturation, and contrast to simulate different lighting conditions.</p> </li> <li> <p>Translation</p> <p></p> <p>Translation shifts the image along the X and Y axes. Useful when subjects appear in different positions.</p> </li> <li> <p>Cutout</p> <p></p> <p>Cutout masks random regions of the image, helping the model learn to focus on different parts of the image.</p> </li> </ul>"},{"location":"training-aug/#ada-transformation-previews","title":"ADA Transformation Previews","text":"<ul> <li> <p>Blit (b)</p> <p></p> <p>Pixel blitting includes flips, 90\u00b0 interval rotations, and translation operations.</p> </li> <li> <p>Geometric (g)</p> <p></p> <p>General geometric transformations include zoom, rotation (up to 360\u00b0), and stretching/squishing.</p> </li> <li> <p>Colour (c)</p> <p></p> <p>Colour transformations simulate different lighting through brightness, contrast, and saturation changes.</p> </li> <li> <p>Filter (f)</p> <p></p> <p>Image-space filtering includes sharpening and blurring operations to adjust textures.</p> </li> <li> <p>Noise (n) &amp; Cutout (c)</p> <p></p> <p>Image-space corruptions include adding noise and masking random regions with patches.</p> </li> </ul>"},{"location":"windows-requirements/","title":"Windows Requirements","text":"<p>Windows users need to have both Microsoft C++ Build Tools and CUDA 12.8+ Development and Runtime libraries installed and properly configured for optimal performance. Failure to do so can slow down training performance by ~250% (3.5x slower).</p>"},{"location":"windows-requirements/#visual-studio-2022-c-build-tools","title":"Visual Studio 2022 C++ Build Tools","text":"<p>Download link</p> <p>Minimum components:</p> <ul> <li>Desktop development with C++</li> </ul> <p></p>"},{"location":"windows-requirements/#cuda-128-or-newer","title":"CUDA 12.8 or newer","text":"<p>(Download link)</p> <p>Minimum components:</p> <ul> <li>CUDA Development</li> <li>CUDA Runtime</li> </ul> <p> </p>"},{"location":"windows-requirements/#environment-variables","title":"Environment variables","text":"<p>While CUDA should automatically be added to your system environment variables, you will still need to install the Microsoft C++ Build Tools variables manually.</p> <ul> <li> <p>In the Windows start menu, type \"environment\" and click \"Edit the system environment variables\", then click on the \"Environment Variables...\" button.</p> </li> <li> <p>In the \"System variables\" section, find the <code>Path</code> variable and double click on it</p> </li> </ul> <p></p> <ul> <li>Finally add a new line with value <code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.44.35207\\bin\\Hostx64\\x64</code></li> </ul> <p></p> <ul> <li>(OPTIONAL) If you want to verify that the dependencies are properly configured, in the Windows start menu, type \"terminal\" and click \"Terminal\", then first type <code>where.exe cl.exe</code> and enter, then <code>where.exe nvcc.exe</code> and enter. In both case it should retun the path as configured in the previous step.</li> </ul> <p></p>"}]}